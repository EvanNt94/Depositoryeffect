import torch
from tradx.nn.llm.MemoryModule import MemoryModule


class LLMWrapper:
    def __init__(self, model_name: str, device: str):
        self.device = device
        self.model_name = model_name
        self.llm = self.load_model()
        self.dim = self.get_embedding_dim()
        self.memory = MemoryModule(dim=self.dim, device=device)

    def load_model(self): ...
    def get_embedding_dim(self): ...
    def embed(self, text: str) -> torch.Tensor: ...
    def forward(self, text: str) -> torch.Tensor: ...
    def surprise(self, text: str) -> float: ...
    def update_memory(self, text: str) -> bool: ...