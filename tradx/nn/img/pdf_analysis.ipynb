{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a2/anaconda3/envs/trade/lib/python3.12/site-packages/doclayout_yolo/nn/tasks.py:733: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from doclayout_yolo import YOLOv10\n",
    "path = '/Users/a2/Desktop/Bildschirmfoto 2024-11-01 um 00.46.51.png'\n",
    "model = YOLOv10(\"/Users/a2/.cache/huggingface/hub/models--juliozhao--DocLayout-YOLO-DocStructBench/snapshots/8c3299a30b8ff29a1503c4431b035b93220f7b11/doclayout_yolo_docstructbench_imgsz1024.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from doclayout_yolo.engine.results import Results\n",
    "def generate(image, model:YOLOv10):\n",
    "        \n",
    "    # Load the pre-trained model\n",
    "\n",
    "\n",
    "    det_res = model.predict(\n",
    "        image,   # Image to predict\n",
    "        imgsz=1024,        # Prediction image size\n",
    "        conf=0.2,          # Confidence threshold\n",
    "        device=\"cpu\",    # Device to use (e.g., 'cuda:0' or 'cpu')\n",
    "        verbose=False\n",
    "    )   \n",
    "    return det_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{0: 'title', 1: 'plain text', 2: 'abandon', 3: 'figure', 4: 'figure_caption', 5: 'table', 6: 'table_caption', 7: 'table_footnote', 8: 'isolate_formula', 9: 'formula_caption'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### glaube hier wird immer noch die nächste caption gesucht obwohl diese näher an einer anderen box ist.\n",
    "\n",
    "def match_table_caption_to_table(inputs):\n",
    "    \"\"\"\n",
    "    Matches table captions to their corresponding tables based on proximity.\n",
    "\n",
    "    Parameters:\n",
    "    inputs (list): A list of dictionaries containing detected elements with their bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of dictionaries with 'caption' and 'table' keys, representing matched pairs.\n",
    "    \"\"\"\n",
    "    tables = [item for item in inputs if item['name'] == 'table']\n",
    "    captions = [item for item in inputs if item['name'] == 'table_caption']\n",
    "\n",
    "    matches = []\n",
    "\n",
    "    for caption in captions:\n",
    "        caption_box = caption['box']\n",
    "        min_distance = float('inf')\n",
    "        matched_table = None\n",
    "\n",
    "        for table in tables:\n",
    "            table_box = table['box']\n",
    "\n",
    "            # Compute vertical distance between caption and table\n",
    "            if caption_box['y2'] <= table_box['y1']:\n",
    "                distance = table_box['y1'] - caption_box['y2']  # Caption is above the table\n",
    "            elif caption_box['y1'] >= table_box['y2']:\n",
    "                distance = caption_box['y1'] - table_box['y2']  # Caption is below the table\n",
    "            else:\n",
    "                distance = 0  # Overlapping vertically\n",
    "\n",
    "            # Compute horizontal overlap\n",
    "            x_overlap = max(0, min(caption_box['x2'], table_box['x2']) - max(caption_box['x1'], table_box['x1']))\n",
    "\n",
    "            # Only consider captions that horizontally overlap with the table\n",
    "            if x_overlap > 0 and distance < min_distance:\n",
    "                min_distance = distance\n",
    "                matched_table = table\n",
    "\n",
    "        if matched_table:\n",
    "            matches.append({'caption': caption, 'table': matched_table})\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def extract_sub_image(path:str, coordinates)->None:\n",
    "    image = Image.open(path)\n",
    "    return image.crop(coordinates)\n",
    "class TableImage:\n",
    "    def __init__(self, path, x1, y1, x2, y2, caption=\"\") -> None:\n",
    "        self.coordinates = (int(x1), int(y1), int(x2+1), int(y2+1))\n",
    "        self.img:Image = extract_sub_image(path, self.coordinates)\n",
    "        self.caption=caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "res = json.loads(generate(path, model)[0].tojson())\n",
    "tables = [x for x in res if x[\"name\"]==\"table\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "box = tables[0][\"box\"]\n",
    "TableImage(path, box[\"x1\"], box[\"y1\"], box[\"x2\"], box[\"y2\"]).img.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'table',\n",
       " 'class': 5,\n",
       " 'confidence': 0.97744,\n",
       " 'box': {'x1': 69.12816, 'y1': 756.10773, 'x2': 737.65228, 'y2': 1068.82764}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doclayout_yolo.engine.results.Results"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'table',\n",
       "  'class': 5,\n",
       "  'confidence': 0.97744,\n",
       "  'box': {'x1': 69.12816, 'y1': 756.10773, 'x2': 737.65228, 'y2': 1068.82764}},\n",
       " {'name': 'table',\n",
       "  'class': 5,\n",
       "  'confidence': 0.97463,\n",
       "  'box': {'x1': 69.0334, 'y1': 277.42413, 'x2': 822.0285, 'y2': 503.14926}},\n",
       " {'name': 'plain text',\n",
       "  'class': 1,\n",
       "  'confidence': 0.96961,\n",
       "  'box': {'x1': 65.7185, 'y1': 147.82533, 'x2': 979.5835, 'y2': 238.79291}},\n",
       " {'name': 'plain text',\n",
       "  'class': 1,\n",
       "  'confidence': 0.96861,\n",
       "  'box': {'x1': 66.33956,\n",
       "   'y1': 1172.43652,\n",
       "   'x2': 977.52289,\n",
       "   'y2': 1230.44641}},\n",
       " {'name': 'title',\n",
       "  'class': 0,\n",
       "  'confidence': 0.93105,\n",
       "  'box': {'x1': 65.85232, 'y1': 563.1853, 'x2': 234.17126, 'y2': 586.26813}},\n",
       " {'name': 'title',\n",
       "  'class': 0,\n",
       "  'confidence': 0.92302,\n",
       "  'box': {'x1': 66.71995, 'y1': 115.30495, 'x2': 151.06572, 'y2': 138.37685}},\n",
       " {'name': 'plain text',\n",
       "  'class': 1,\n",
       "  'confidence': 0.87674,\n",
       "  'box': {'x1': 66.37093, 'y1': 593.40167, 'x2': 964.9295, 'y2': 685.55145}},\n",
       " {'name': 'table_caption',\n",
       "  'class': 6,\n",
       "  'confidence': 0.86804,\n",
       "  'box': {'x1': 66.38075, 'y1': 725.89282, 'x2': 484.44595, 'y2': 749.20557}},\n",
       " {'name': 'plain text',\n",
       "  'class': 1,\n",
       "  'confidence': 0.83513,\n",
       "  'box': {'x1': 65.4129, 'y1': 593.67456, 'x2': 965.00293, 'y2': 685.27313}},\n",
       " {'name': 'table_footnote',\n",
       "  'class': 7,\n",
       "  'confidence': 0.71636,\n",
       "  'box': {'x1': 67.43879, 'y1': 503.21841, 'x2': 684.05096, 'y2': 522.30969}},\n",
       " {'name': 'abandon',\n",
       "  'class': 2,\n",
       "  'confidence': 0.65953,\n",
       "  'box': {'x1': 835.43298, 'y1': 27.08222, 'x2': 993.68292, 'y2': 70.92496}},\n",
       " {'name': 'table_footnote',\n",
       "  'class': 7,\n",
       "  'confidence': 0.60354,\n",
       "  'box': {'x1': 66.6855, 'y1': 1068.53711, 'x2': 685.02234, 'y2': 1088.19189}},\n",
       " {'name': 'plain text',\n",
       "  'class': 1,\n",
       "  'confidence': 0.53968,\n",
       "  'box': {'x1': 66.47595, 'y1': 247.68362, 'x2': 483.36893, 'y2': 269.96561}},\n",
       " {'name': 'abandon',\n",
       "  'class': 2,\n",
       "  'confidence': 0.5031,\n",
       "  'box': {'x1': 87.13834, 'y1': 26.77563, 'x2': 790.36896, 'y2': 70.10345}},\n",
       " {'name': 'abandon',\n",
       "  'class': 2,\n",
       "  'confidence': 0.48321,\n",
       "  'box': {'x1': 844.16089, 'y1': 30.57914, 'x2': 979.83002, 'y2': 69.42999}}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypdf\n",
    "\n",
    "pp = \"/Users/a2/code/fin/trade/test_docs/SELECTED TEXTILE IND. ASSOC. S.A. 01 - 11 - 2024.pdf\"\n",
    "doc =  pypdf.PdfReader(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form Κ3-02-025\n",
      "DATE: 01/11/2024 TIME: 12:00:00 πμ\n",
      "α/α\n",
      "Ελληνικό \n",
      "Σύμβολο / Greek \n",
      "Symbol\n",
      "Αγγλικό \n",
      "Σύμβολο / \n",
      "English Symbol\n",
      "Κωδικός ISIN / \n",
      "ISIN Code\n",
      "Ενέργεια επί του Χρεογράφου / \n",
      "Action\n",
      "Ωρα / \n",
      "Time\n",
      "1 ΕΠΙΛΚ EPIL GRS045003001 Προσωρινή Αναστολή / Halt 12:00:00 πμ\n",
      "Select one\n",
      "Select one\n",
      "Select one\n",
      "Select one\n",
      "Select one\n",
      "Μετοχή/ \n",
      "Share: Επικεφαλίδα / Title:\n",
      "Κείμενο / Text:\n",
      "Από 01/11/2024, κατόπιν αιτήματος της Επιτροπής Κεφαλαιαγοράς, αναστέλλεται προσωρινά η διαπραγμάτευση των μετοχών της εταιρίας «ΕΠΙΛΕΚΤΟΣ \n",
      "ΚΛΩΣ/ΡΓΙΑ Α.Ε.Β.Ε.» (GRS045003001) στο Χρηματιστήριο Αθηνών. \n",
      "As of November 1, 2024, after consideration of the relevant request of the Hellenic Capital Market Commission, the trading of the shares of the company  \n",
      "“SELECTED TEXTILE IND. ASSOC. S.A.” (GRS045003001) is temporarily suspended on ATHEX.\n",
      "Select one\n",
      "Select one\n",
      "Select one\n",
      "Περιγραφή / Description\n",
      "ΕΠΙΛΚ / EPIL Αναστολή / Halt\n",
      "Select one\n",
      "ΔΙΑΚΟΠΗ / ΕΠΑΝΕΝΑΡΞΗ ΑΞΙΟΓΡΑΦΟΥ\n",
      "HALT / RESUME SYMBOL\n",
      "Διακοπή & Επαναφορά Αξιογράφου / Halt & Resume Symbol\n",
      "Αγορά / \n",
      "Market\n",
      " Επιτήρησης / Surveillance \n",
      "Select one\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc.get_page(0).extract_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
