{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## todo: referrer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, time\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_resp(resp:requests.Response):\n",
    "    headers = resp.headers\n",
    "    pattern = r\"^([a-z]+)/([a-z]+)\"\n",
    "    try:\n",
    "        m = re.match(pattern,headers['Content-Type'])\n",
    "    except KeyError:\n",
    "        return None, None\n",
    "    typ = m.group(1)\n",
    "    filetype = m.group(2)\n",
    "    return typ, filetype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_links(page_source):\n",
    "    # Regex zum Extrahieren von href-Werten\n",
    "    href_pattern = r'href=[\"\\'](.*?)[\"\\']'\n",
    "    # Alle Übereinstimmungen finden\n",
    "    hrefs = re.findall(href_pattern, page_source)\n",
    "    return hrefs\n",
    "def gehört_url_zu_hauptdomain(url, hauptdomain):\n",
    "    parsed_url = urlparse(url)\n",
    "    # Überprüfe, ob die Hauptdomain am Ende des Hostnamens steht\n",
    "    netloc = parsed_url.netloc\n",
    "    parts = netloc.split(\".\")\n",
    "    tl_domain = parts[-1]\n",
    "    sl_domain = parts[-2]\n",
    "    return hauptdomain == sl_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_public_links(base_url:str)->dict[str, list[str]]:\n",
    "    root_domain = urlparse(base_url).netloc.split(\".\")[-2]\n",
    "    res_d = {\"text/html\": [base_url]}\n",
    "    visited_set = {base_url}\n",
    "    referrer_dict = {base_url: None}\n",
    "    links = extract_all_links(requests.get(base_url).text)\n",
    "    queue_set = set(links)\n",
    "    for link in links:\n",
    "        referrer_dict[link] = [base_url]\n",
    "    while queue_set:\n",
    "        current_elem = queue_set.pop()\n",
    "        if current_elem.startswith(\"mailto:\"):\n",
    "            continue\n",
    "        if not current_elem.startswith(\"https://\"):\n",
    "            if current_elem.startswith(\"/\"):\n",
    "                current_elem = base_url+current_elem\n",
    "            else:\n",
    "                current_elem = base_url+\"/\"+current_elem\n",
    "\n",
    "        pattern = r'^(https?:\\/\\/.*?)(?=(https?:\\/\\/|$))'\n",
    "        current_elem = re.match(pattern, current_elem).group(1)\n",
    "        \n",
    "        time.sleep(5)\n",
    "        print(current_elem)\n",
    "        print(f\"Besuche: {current_elem}\")\n",
    "        try:\n",
    "            resp = requests.get(current_elem)\n",
    "        except Exception as e:\n",
    "            print(f\"Fehler beim Abrufen von {current_elem}: {e}\")\n",
    "            continue\n",
    "\n",
    "        visited_set.add(current_elem)\n",
    "        keys = res_d.keys()\n",
    "        tf = _eval_resp(resp)\n",
    "        content = \"/\".join(tf)\n",
    "        if content not in keys:\n",
    "            res_d[content] = []\n",
    "        res_d[content].append(current_elem)\n",
    "        if tf != (\"text\", \"html\"):\n",
    "            continue\n",
    "        if not gehört_url_zu_hauptdomain(current_elem, root_domain):\n",
    "            continue\n",
    "        new_links = set(extract_all_links(resp.text))\n",
    "        links_not_visited = new_links - visited_set\n",
    "        for link in links_not_visited:\n",
    "            if link in referrer_dict:\n",
    "                referrer_dict[link].append(current_elem)\n",
    "            else:\n",
    "                referrer_dict[link] = [current_elem]\n",
    "\n",
    "        queue_set.update(links_not_visited)\n",
    "    return res_d, referrer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.athexgroup.gr/web/guest/companies-information-corporate-actions/-/asset_publisher/E8Xk2zINhmZA/document/id/7590823?controlPanelCategory=portlet_101_INSTANCE_E8Xk2zINhmZA&redirect=https%3A%2F%2Fwww.athexgroup.gr%2Fweb%2Fguest%2Fcompanies-information-corporate-actions%3Fp_p_id%3D101_INSTANCE_E8Xk2zINhmZA%26p_p_lifecycle%3D0%26p_p_state%3Dnormal%26p_p_mode%3Dview%26controlPanelCategory%3Dportlet_101_INSTANCE_E8Xk2zINhmZA%26_101_INSTANCE_E8Xk2zINhmZA_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Date': 'Mon, 11 Nov 2024 14:31:26 GMT', 'Server': 'Apache', 'Strict-Transport-Security': 'max-age=63072000; includeSubdomains;', 'X-Frame-Options': 'SAMEORIGIN', 'Expires': 'Thu, 01 Jan 1970 00:00:00 GMT', 'Cache-Control': 'private, no-cache, no-store, must-revalidate', 'Pragma': 'no-cache', 'Content-Type': 'text/html;charset=UTF-8', 'Set-Cookie': 'JSESSIONID=E934EBB7C0C00B0FCFC4C2DB4ECE8365.jvm2; Path=/; Secure; HttpOnly;HttpOnly;Secure, GUEST_LANGUAGE_ID=en_US; Expires=Tue, 11-Nov-2025 14:31:26 GMT; Path=/; Secure;HttpOnly;Secure, COOKIE_SUPPORT=true; Expires=Tue, 11-Nov-2025 14:31:26 GMT; Path=/; Secure;HttpOnly;Secure, visid_incap_2225312=LR1cARm0Tb2h9t2an36wLjEVMmcAAAAAQUIPAAAAAAAdSzyW9OOwh/tgKvsggNkz; expires=Mon, 10 Nov 2025 23:10:41 GMT; HttpOnly; path=/; Domain=.athexgroup.gr; Secure; SameSite=None, nlbi_2225312=lfT5Uiu2pU1sr8KYVgaPnAAAAAD7N4wZ9vTtsycOxMEvrzkM; HttpOnly; path=/; Domain=.athexgroup.gr; Secure; SameSite=None, incap_ses_8219_2225312=eaBBHDCRkyy/1Uo/CcEPcj4VMmcAAAAAFzCO4ZaIzGtBHgioOIL+oQ==; path=/; Domain=.athexgroup.gr; Secure; SameSite=None', 'X-XSS-Protection': '1; mode=block', 'Keep-Alive': 'timeout=5, max=100', 'Connection': 'Keep-Alive', 'Transfer-Encoding': 'chunked', 'X-CDN': 'Imperva', 'Content-Encoding': 'gzip', 'X-Iinfo': '11-123880819-123880822 NNYY CT(50 105 0) RT(1731335485927 13) q(0 0 0 0) r(5 5) U12'}\n"
     ]
    }
   ],
   "source": [
    "print(resp.headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"application/pdf\"\n",
    "pattern = r\"^([a-z]+)/([a-z]+)\"\n",
    "m = re.match(pattern=pattern, string=s)\n",
    "m.group(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trade",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
